{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import operator\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats.stats import pearsonr\n",
    "import math\n",
    "\n",
    "import mpld3\n",
    "from mpld3 import plugins\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msg_id_regex = re.compile(r'^Message\\-ID:\\s+<(.*)>$', re.MULTILINE)\n",
    "date_regex = re.compile(r'^Date:\\s+(.*)$', re.MULTILINE)\n",
    "date_extract_regex = re.compile(r'\\d{1,2}\\s\\w{3}\\s\\d{4}')\n",
    "\n",
    "header_regex = re.compile(r'Message-ID:.*((.|\\n)*)X-From', re.MULTILINE)\n",
    "\n",
    "subject_regex = re.compile(r'^Subject:(.*)\\n', re.MULTILINE)\n",
    "\n",
    "mail_sender_regex =  re.compile(r'^From:\\s+(.*@[\\w\\-\\.]+\\.\\w+)$', re.MULTILINE)\n",
    "#mail_recipient_regex = re.compile(r'^To:\\s+(.*@[\\w\\-\\.]+\\.\\w+)$', re.MULTILINE)\n",
    "to_recipient_regex = re.compile(r'^To:\\s((.|\\n)*?)Subject', re.MULTILINE)\n",
    "cc_recipient_regex = re.compile(r'^[cC]{2}:\\s((.|\\n)*)Mime', re.MULTILINE)\n",
    "bcc_recipient_regex = re.compile(r'^Bcc:\\s((.|\\n)*)X-From', re.MULTILINE)\n",
    "\n",
    "forwarded_regex = re.compile(r'(\\n\\s?--{2,})', re.MULTILINE)\n",
    "regular_body_regex = re.compile(r'^X-FileName:.*((.|\\n)*)', re.MULTILINE)\n",
    "forwarded_body_regex = re.compile(r'^X-FileName:.*((.|\\n)+?)(?=\\n\\s?--{2,})', re.MULTILINE)\n",
    "\n",
    "date_regex = re.compile(r'^Date:\\s+(.*)$', re.MULTILINE)\n",
    "date_extract_regex = re.compile(r'\\d{1,2}\\s\\w{3}\\s\\d{4}')\n",
    "day_of_the_week_extract_regex = re.compile(r'^(\\w{2,3}),')\n",
    "time_extract_regex = re.compile(r'(\\d{2}:\\d{2}:\\d{2})')\n",
    "time_zone_extract_regex = re.compile(r'[+-]{1}\\d{4}')\n",
    "day_extract_regex = re.compile(r'^\\d{1,2}')\n",
    "month_extract_regex = re.compile(r'\\s(\\w{2,3})\\s')\n",
    "year_extract_regex = re.compile(r'\\d{4}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING CORPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Parser Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will go through the email folders and find the sent email folders. We are going to work only over the sent emails to try to detect the POI from the non-POI.\n",
    "\n",
    "The function takes one argument, the number of folders to process. This one was introduced for debugging purposes only, when I wanted to parse only a handful of folders to see if the function worked correctly. When used for the final run, I just pass a number larger than the total number of folders.\n",
    "\n",
    "This function will open each email, parse its body (It will try to isolate only what was written by the user, ie discard any forwarded part) and append it to a corpus list. It will also create a label list, associating each entry of the corpus list with its user. The return value is a list made of two elements, the corpus and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_corpus(number_of_users):\n",
    "    emails_path = \"C:\\\\Users\\\\e_tak_000\\\\Documents\\\\GitHub\\\\ud120-projects\\\\enron_mail_20150507\\\\enron_mail_20150507\\\\maildir\"\n",
    "    sent_folders_set = [\"_sent_mail\", \"sent\", \"_sent\", \"sent_items\"]\n",
    "    corpus = []\n",
    "    labels = []\n",
    "\n",
    "    users_folder = [f for f in listdir(emails_path) if isdir(join(emails_path, f))]\n",
    "    counter = 0\n",
    "\n",
    "    root_folders_list = []\n",
    "    #Create the instance for all users\n",
    "    for top_folder in users_folder:\n",
    "        root_folders_list.append(top_folder)\n",
    "        \n",
    "    #loop over each email owner\n",
    "    for user_root_folder in root_folders_list:\n",
    "        if counter > number_of_users:\n",
    "            break\n",
    "        counter += 1\n",
    "        \n",
    "        print counter, user_root_folder\n",
    "        user_full_path = join(emails_path, user_root_folder)\n",
    "        inner_folders = [f for f in listdir(user_full_path) if isdir(join(user_full_path, f))]\n",
    "        #Loop over all subfolders\n",
    "        for current_folder in inner_folders:\n",
    "            #If this folder is in the sent documents, start processing\n",
    "            if current_folder in sent_folders_set:\n",
    "                folder_full_path = join(user_full_path, current_folder)\n",
    "                current_folder_files = [f for f in listdir(folder_full_path) if isfile(join(folder_full_path, f)) ]\n",
    "                #Loop over all sent emails\n",
    "                for current_file in current_folder_files:\n",
    "\n",
    "                    msg = open(join(folder_full_path, current_file), 'r').read()\n",
    "                    \n",
    "                    # Get the message body\n",
    "                    if re.findall(forwarded_regex, msg):\n",
    "                        #print current_file\n",
    "                        msg_body = re.findall(forwarded_body_regex, msg)\n",
    "                        #print \"forwarded Message __________________________________\"\n",
    "                    else:\n",
    "                        msg_body = re.findall(regular_body_regex, msg)\n",
    "\n",
    "                    if len(msg_body) < 1:\n",
    "                        print \"Empty Message In \", current_file\n",
    "                        print msg_body\n",
    "                    else:\n",
    "                        msg_body = msg_body[0][0]\n",
    "                        \n",
    "                    corpus.append(msg_body)\n",
    "                    labels.append(user_root_folder)\n",
    "                    \n",
    "    return [corpus, labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Corpus from the Email Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 allen-p\n",
      "2 arnold-j\n",
      "3 arora-h\n",
      "4 badeer-r\n",
      "5 bailey-s\n",
      "6 bass-e\n",
      "7 baughman-d\n",
      "8 beck-s\n",
      "9 benson-r\n",
      "10 blair-l\n",
      "11 brawner-s\n",
      "12 buy-r\n",
      "13 campbell-l\n",
      "14 carson-m\n",
      "15 cash-m\n",
      "16 causholli-m\n",
      "17 corman-s\n",
      "18 crandell-s\n",
      "19 cuilla-m\n",
      "20 dasovich-j\n",
      "21 davis-d\n",
      "22 dean-c\n",
      "23 delainey-d\n",
      "24 derrick-j\n",
      "25 dickson-s\n",
      "26 donoho-l\n",
      "27 donohoe-t\n",
      "28 dorland-c\n",
      "29 ermis-f\n",
      "30 farmer-d\n",
      "31 fischer-m\n",
      "32 forney-j\n",
      "33 fossum-d\n",
      "34 gang-l\n",
      "35 gay-r\n",
      "36 geaccone-t\n",
      "37 germany-c\n",
      "38 gilbertsmith-d\n",
      "39 giron-d\n",
      "40 griffith-j\n",
      "41 grigsby-m\n",
      "42 guzman-m\n",
      "43 haedicke-m\n",
      "44 hain-m\n",
      "45 harris-s\n",
      "46 hayslett-r\n",
      "47 heard-m\n",
      "48 hendrickson-s\n",
      "49 hernandez-j\n",
      "50 hodge-j\n",
      "51 holst-k\n",
      "52 horton-s\n",
      "53 hyatt-k\n",
      "54 hyvl-d\n",
      "55 jones-t\n",
      "56 kaminski-v\n",
      "57 kean-s\n",
      "58 keavey-p\n",
      "59 keiser-k\n",
      "60 king-j\n",
      "61 kitchen-l\n",
      "62 kuykendall-t\n",
      "63 lavorato-j\n",
      "64 lay-k\n",
      "65 lenhart-m\n",
      "66 lewis-a\n",
      "67 linder-e\n",
      "68 lokay-m\n",
      "69 lokey-t\n",
      "70 love-p\n",
      "71 lucci-p\n",
      "72 maggi-m\n",
      "73 mann-k\n",
      "74 martin-t\n",
      "75 may-l\n",
      "76 mccarty-d\n",
      "77 mcconnell-m\n",
      "78 mckay-b\n",
      "79 mckay-j\n",
      "80 mclaughlin-e\n",
      "81 merriss-s\n",
      "82 meyers-a\n",
      "83 mims-thurston-p\n",
      "84 motley-m\n",
      "85 neal-s\n",
      "86 nemec-g\n",
      "87 panus-s\n",
      "88 parks-j\n",
      "89 pereira-s\n",
      "90 perlingiere-d\n",
      "91 phanis-s\n",
      "92 pimenov-v\n",
      "93 platter-p\n",
      "94 presto-k\n",
      "95 quenet-j\n",
      "96 quigley-d\n",
      "97 rapp-b\n",
      "98 reitmeyer-j\n",
      "99 richey-c\n",
      "100 ring-a\n",
      "101 ring-r\n",
      "102 rodrique-r\n",
      "103 rogers-b\n",
      "104 ruscitti-k\n",
      "105 sager-e\n",
      "106 saibi-e\n",
      "107 salisbury-h\n",
      "108 sanchez-m\n",
      "109 sanders-r\n",
      "110 scholtes-d\n",
      "111 schoolcraft-d\n",
      "112 schwieger-j\n",
      "113 scott-s\n",
      "114 semperger-c\n",
      "115 shackleton-s\n",
      "116 shankman-j\n",
      "117 shapiro-r\n",
      "118 shively-h\n",
      "119 skilling-j\n",
      "120 slinger-r\n",
      "121 smith-m\n",
      "122 solberg-g\n",
      "123 south-s\n",
      "124 staab-t\n",
      "125 stclair-c\n",
      "126 steffes-j\n",
      "127 stepenovitch-j\n",
      "128 stokley-c\n",
      "129 storey-g\n",
      "130 sturm-f\n",
      "131 swerzbin-m\n",
      "132 symes-k\n",
      "133 taylor-m\n",
      "134 tholt-j\n",
      "135 thomas-p\n",
      "136 townsend-j\n",
      "137 tycholiz-b\n",
      "138 ward-k\n",
      "139 watson-k\n",
      "140 weldon-c\n",
      "141 whalley-g\n",
      "142 whalley-l\n",
      "143 white-s\n",
      "144 whitt-m\n",
      "145 williams-j\n",
      "146 williams-w3\n",
      "147 wolfe-j\n",
      "148 ybarbo-p\n",
      "149 zipper-a\n",
      "150 zufferli-j\n",
      "Corpus built in 1190.5s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "training, labels = build_corpus(200)\n",
    "print \"Corpus built in %0.1fs\" % (time() - t0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Just in case, keep an original copy of the corpus aside:\n",
    "backup_corpus_data = [training, labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Labels into Booleans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boolean values are True for being a POI and False for non-POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_list = ['lay-k', 'skilling-j', 'forney-j', 'delainey-d']\n",
    "\n",
    "poi_labels = []\n",
    "\n",
    "for l in labels:\n",
    "    if l in poi_list:\n",
    "        status = True\n",
    "    else:\n",
    "        status = False\n",
    "    poi_labels.append(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert it into numpy array\n",
    "poi_labels_array = np.asarray(poi_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126462L,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poi_labels_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NATURAL LANGUAGE PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I have created this additional stop words list, just in case in the future work I would watnt to remove some words and see how \n",
    "#this will affect the model. But for now, this list is empty\n",
    "my_additional_stop_words = ()\n",
    "\n",
    "#Create the stop words list to be filtered from the corpus\n",
    "stop_words = ENGLISH_STOP_WORDS.union(my_additional_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Term Frequency Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=1, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Emails Body List into TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus transformed in 11.9s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "X = tfidf_vectorizer.fit_transform(training)\n",
    "print \"Corpus transformed in %0.1fs\" % (time() - t0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126462, 94185)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126462"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(poi_labels_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESTIMATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratify-Split the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, # X\n",
    "                                                    poi_labels_array, # y \n",
    "                                                    test_size=0.2, # Keep 20% for testing\n",
    "                                                    random_state=42, # To be able to reproduce the results\n",
    "                                                    stratify=poi_labels_array) #Stratified split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stratified_k_fold = cross_validation.StratifiedKFold(y_train, n_folds= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = [ clf.fit(X_train[train], y_train[train]).score(X_train[test], y_train[test]) for train, test in stratified_k_fold ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.97262304803320809,\n",
       " 0.97311721684127295,\n",
       " 0.97311721684127295,\n",
       " 0.97331224671345262,\n",
       " 0.97321340318276173,\n",
       " 0.97291687259068893,\n",
       " 0.97281534203242392,\n",
       " 0.97321075523922496,\n",
       " 0.9733096085409253,\n",
       " 0.97301304863582438]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97307555450124539"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am extremely impressed, to say the least, about the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
